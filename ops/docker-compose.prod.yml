# Production docker-compose for Datasetto
#
# Architecture Overview:
# - caddy: The public HTTP/HTTPS entry point (ports 80/443). Handles SSL and routing.
# - turn: WebRTC relay server. Exposes UDP/TCP ports directly for media relay.
# - rtmp: RTMP ingestion server. Exposes port 1935 for OBS/streaming software.
# - server: Node.js backend. Internal only.
# - client: Static frontend files served by Nginx. Internal only.
#
# Networking:
# All services share a private bridge network 'app-network'.
# Only 'caddy', 'turn', and 'rtmp' expose ports to the host.

services:
  turn:
    build:
      context: .
      dockerfile: Dockerfile.turn
    container_name: turn-server
    restart: unless-stopped
    networks:
      - app-network
    ports:
      - "${TURN_PORT:-3478}:${TURN_PORT:-3478}"
      - "${TURN_PORT:-3478}:${TURN_PORT:-3478}/udp"
      - '${TURN_MIN_PORT:-49160}-${TURN_MAX_PORT:-49200}:${TURN_MIN_PORT:-49160}-${TURN_MAX_PORT:-49200}/udp'
    env_file:
      - .env
    volumes:
      - ./turn-entrypoint.sh:/usr/local/bin/turn-entrypoint.sh:ro
    entrypoint: ["/bin/sh", "/usr/local/bin/turn-entrypoint.sh"]
    deploy:
      resources:
        limits:
          cpus: '0.35'
          memory: 192M
        reservations:
          cpus: '0.08'
          memory: 64M

  # NGINX RTMP Server (Custom build with CORS)
  rtmp:
    build:
      context: .
      dockerfile: Dockerfile.rtmp
    container_name: rtmp-server
    restart: unless-stopped
    depends_on:
      - server
    ports:
      - "1935:1935"  # RTMP ingestion (must be public for OBS)
    # Keep HLS segments on a ramdisk for faster writes
    tmpfs:
      - /tmp/hls:rw,size=512m,mode=1777
    networks:
      - app-network
    deploy:
      resources:
        limits:
          cpus: '1.6'
          memory: 896M
        reservations:
          cpus: '0.6'
          memory: 384M
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
  
  # Backend Server (Node.js + Socket.IO)
  server:
    build:
      context: ../server
      dockerfile: Dockerfile.prod
    container_name: backend-server
    restart: unless-stopped
    # No external ports - accessed via reverse proxy
    env_file:
      - .env
    environment:
      - NODE_ENV=production
      - PORT=${PORT:-4000}
      - HOST=0.0.0.0
      - CORS_ORIGIN=${CORS_ORIGIN:-*}
      - HLS_BASE_URL=${HLS_BASE_URL:-http://localhost/hls}
      - MAX_CONNECTIONS_PER_IP=${MAX_CONNECTIONS_PER_IP:-10}
      - MAX_CHANNELS=${MAX_CHANNELS:-50}
      - MAX_USERS_PER_CHANNEL=${MAX_USERS_PER_CHANNEL:-50}
      # Redis connection for persistent storage
      - REDIS_URL=redis://redis:6379
      # Release directories for the downloads API
      - RELEASES_DESKTOP_DIR=/releases/desktop
      - RELEASES_MOBILE_DIR=/releases/mobile
    volumes:
      # Mount release directories for the downloads API
      - ../desktop/release:/releases/desktop:ro
      - ../mobile/release:/releases/mobile:ro
      # Persistent storage volume for file-based fallback
      - ./data/storage:/app/storage
    networks:
      - app-network
    depends_on:
      - turn
      - redis
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.15'
          memory: 128M
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:4000/health', (r) => { process.exit(r.statusCode === 200 ? 0 : 1); }).on('error', () => process.exit(1));"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  
  # Frontend (Static files served by nginx)
  client:
    build:
      context: ../client
      dockerfile: Dockerfile.prod
    container_name: frontend-client
    restart: unless-stopped
    # Runtime configuration via environment variables
    env_file:
      - .env
    environment:
      - VITE_SERVER_URL=${SERVER_URL:-/}
      - VITE_HLS_BASE_URL=${HLS_BASE_URL:-/hls}
      - VITE_API_BASE_URL=${API_BASE_URL:-/api}
      - VITE_WEBRTC_ICE_SERVERS=${VITE_WEBRTC_ICE_SERVERS:-}
      - VITE_TURN_URL=${VITE_TURN_URL:-}
      - VITE_TURN_USERNAME=${VITE_TURN_USERNAME:-}
      - VITE_TURN_CREDENTIAL=${VITE_TURN_CREDENTIAL:-}
      - VITE_VOICE_OPUS_BITRATE=${VITE_VOICE_OPUS_BITRATE:-64000}
      - VITE_VOICE_DTX_ENABLED=${VITE_VOICE_DTX_ENABLED:-true}
      - VITE_VOICE_OPUS_STEREO=${VITE_VOICE_OPUS_STEREO:-false}
      - VITE_VOICE_OPUS_MIN_PTIME=${VITE_VOICE_OPUS_MIN_PTIME:-10}
      - VITE_VOICE_OPUS_MAX_PTIME=${VITE_VOICE_OPUS_MAX_PTIME:-20}
      - VITE_VOICE_OPUS_MAX_PLAYBACK_RATE=${VITE_VOICE_OPUS_MAX_PLAYBACK_RATE:-48000}
      - VITE_VOICE_VAD_THRESHOLD=${VITE_VOICE_VAD_THRESHOLD:-0.07}
      - VITE_MOBILE_DEFAULT_SERVER_URL=${VITE_MOBILE_DEFAULT_SERVER_URL:-}
      - VITE_MOBILE_DEFAULT_HLS_URL=${VITE_MOBILE_DEFAULT_HLS_URL:-}
      - VITE_MOBILE_DEFAULT_RTMP_URL=${VITE_MOBILE_DEFAULT_RTMP_URL:-}
      - VITE_SCREENSHARE_MAX_BITRATE_KBPS=${VITE_SCREENSHARE_MAX_BITRATE_KBPS:-25000}
      - VITE_SCREENSHARE_IDEAL_FPS=${VITE_SCREENSHARE_IDEAL_FPS:-90}
      - VITE_SCREENSHARE_MAX_FPS=${VITE_SCREENSHARE_MAX_FPS:-120}
      # SEO: Site URL for Open Graph, Twitter Cards, JSON-LD
      - SITE_URL=${SITE_URL:-https://datasetto.com}
    # No external ports - accessed via reverse proxy
    networks:
      - app-network
    depends_on:
      - server
      - turn
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 128M
        reservations:
          cpus: '0.05'
          memory: 64M
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:80/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Caddy Reverse Proxy (Automatic HTTPS)
  caddy:
    image: caddy:2.7-alpine
    container_name: caddy-reverse-proxy
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
      - "443:443/udp"
    networks:
      - app-network
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - caddy_data:/data
      - caddy_config:/config
    environment:
      - DOMAIN=${DOMAIN:-localhost}
      - LETSENCRYPT_EMAIL=${LETSENCRYPT_EMAIL:-admin@localhost}
      - CADDY_SITE_ADDRESS=${CADDY_SITE_ADDRESS:-localhost}
    depends_on:
      - client
      - server
      - rtmp
    deploy:
      resources:
        limits:
          cpus: '0.35'
          memory: 160M
        reservations:
          cpus: '0.1'
          memory: 96M
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:2019/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Redis Database (Persistent storage for accounts, messages, sessions)
  redis:
    image: redis:7-alpine
    container_name: redis-db
    restart: unless-stopped
    networks:
      - app-network
    volumes:
      # Persistent volume - survives container destruction
      - ./data/redis:/data
    command: >
      redis-server
      --appendonly yes
      --appendfsync everysec
      --maxmemory 128mb
      --maxmemory-policy allkeys-lru
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 192M
        reservations:
          cpus: '0.08'
          memory: 96M
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

volumes:
  caddy_data:
  caddy_config:

networks:
  app-network:
    driver: bridge
